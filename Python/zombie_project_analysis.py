# -*- coding: utf-8 -*-
"""zombie_project_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oFjS0Gnh3Akqg-VEmp9j9tbb8r0CPk98

# ðŸ§Ÿ Zombie Initiative Detection & Strategic Risk Analysis | Consulting Simulation

This notebook analyzes underperforming "Zombie" initiatives â€” projects that consume resources but fail to deliver measurable results.

It provides actionable insights by detecting financial waste, assessing initiative efficiency, and highlighting projects at risk of underperformance.

---

##  Objectives:
- Detect Zombie initiatives based on:
   - Zero team activity
   - No positive KPI outcomes
   - Budget overspend
- Quantify wasted budget and average loss per Zombie initiative
- Support strategic decision-making for resource optimization

---

##  Tools & Techniques:
- Python (Pandas) for data preparation and business rule application
- Cleaned project datasets aligned with consulting risk analysis standards
- Rule-based risk scoring (no automated ML applied)

---
"""

from google.colab import files

# Upload files from your local machine to Colab
uploaded = files.upload()

import pandas as pd

# Load the uploaded datasets into pandas DataFrames
projects = pd.read_csv('projects_complex_final.csv')
activity = pd.read_csv('activity_logs_complex_final.csv')
kpi = pd.read_csv('kpi_data_complex_final.csv')
sentiment = pd.read_csv('sentiment_data_complex_final.csv')

# Display the first few rows of each DataFrame to inspect the data

projects.head()
activity.head()
kpi.head()
sentiment.head()

# Merge the DataFrames based on 'project_id' to create a comprehensive dataset

merged_df = pd.merge(projects, activity, on='project_id', how='left')
merged_df = pd.merge(merged_df, kpi, on='project_id', how='left')
merged_df = pd.merge(merged_df, sentiment, on='project_id', how='left')

merged_df.head()

# Display information about the merged DataFrame

merged_df.info()

merged_df.head()

# Identify "zombie" projects based on specific criteria:
# 1. Project status is 'Active'
# 2. No hours have been logged (hours_logged == 0)
# 3. The KPI value is less than or equal to 0 (no positive impact)

zombies = merged_df[
    (merged_df['status'] == 'Active') &
    (merged_df['hours_logged'] == 0) &
    (merged_df['kpi_value'] <= 0)
]

zombies.head()

# Calculate key metrics for the identified zombie projects:
# - Total budget wasted across all zombie projects
# - Total count of zombie projects
# - Average wasted budget per zombie project

total_wasted_budget = zombies['budget_usd'].sum()
total_zombie_count = zombies.shape[0]
average_wasted_budget = zombies['budget_usd'].mean()


print(f"Total Zombie Projects: {total_zombie_count}")
print(f"Total Wasted Budget: ${total_wasted_budget:,.2f}")
print(f"Average Wasted Budget per Zombie Project: ${average_wasted_budget:,.2f}")

#Department-Wise Zombie Waste Breakdown
dept_summary = zombies.groupby('department').agg(
    zombie_count = ('project_id', 'count'),
    total_wasted_budget = ('budget_usd', 'sum')
).reset_index()


print(dept_summary)

dept_summary_sorted = dept_summary.sort_values(by='zombie_count', ascending=False)
print(dept_summary_sorted)

# Analyze the monthly trend of zombie projects based on their start date

merged_df['start_date'] = pd.to_datetime(merged_df['start_date'])

zombies = merged_df[
    (merged_df['status'] == 'Active') &
    (merged_df['hours_logged'] == 0) &
    (merged_df['kpi_value'] <= 0)
]

# Extract the month and year from the 'start_date' and store it in a new column 'project_month'

zombies['project_month'] = zombies['start_date'].dt.to_period('M')

# Group the 'zombies' DataFrame by 'project_month' and count the number of zombie projects per month
monthly_trend = zombies.groupby('project_month').agg(
    zombie_count=('project_id', 'count')
).reset_index()


print(monthly_trend)

# Analyze which departments have the most zombie projects and the highest wasted budget

zombies = merged_df[
    (merged_df['status'] == 'Active') &
    (merged_df['hours_logged'] == 0) &
    (merged_df['kpi_value'] <= 0)
]

dept_analysis = zombies.groupby('department').agg(
    zombie_count=('project_id', 'count'),
    total_wasted_budget=('budget_usd', 'sum')
).reset_index()

dept_analysis = dept_analysis.sort_values(by='zombie_count', ascending=False)

print(dept_analysis)

# Section for Predictive Zombie Risk modeling

import pandas as pd

# a new DataFrame 'model_df' with relevant columns for predictive modeling.
model_df = merged_df[['budget_usd', 'employee_sentiment', 'kpi_value', 'status', 'hours_logged']].copy()

# a binary target variable 'zombie_flag':
# 1 if the project is a zombie (Active, 0 hours_logged, kpi_value <= 0), 0 otherwise.
model_df['zombie_flag'] = ((model_df['status'] == 'Active') &
                           (model_df['hours_logged'] == 0) &
                           (model_df['kpi_value'] <= 0)).astype(int)

# Display the head of the 'model_df' to inspect the new column
model_df.head()

# Encode the 'employee_sentiment' categorical variable into numerical representation
# Using a dictionary to map sentiment levels to numbers

sentiment_mapping = {
    'Negative': 0,
    'Neutral': 1,
    'Positive': 2
}

# Apply the mapping
model_df['sentiment_encoded'] = model_df['employee_sentiment'].map(sentiment_mapping)

model_df.drop('employee_sentiment', axis=1, inplace=True)

model_df.head()

# Step 1: Map sentiment to numerical values for risk scoring.
# Here, 'Negative' sentiment is mapped to 1 (indicating higher risk), others to 0.

merged_df['sentiment_encoded'] = merged_df['employee_sentiment'].map({
    'Negative': 1,
    'Neutral': 0,
    'Positive': 0
})

# Calculating a 'total_risk_score' based on multiple factors:

merged_df['total_risk_score'] = (
    (merged_df['budget_usd'] > 1000000).astype(int) +
    (merged_df['sentiment_encoded'] == 1).astype(int) +
    (merged_df['kpi_value'] <= 0).astype(int)
)

model_df = merged_df[['project_id', 'budget_usd', 'sentiment_encoded', 'kpi_value', 'total_risk_score']]

model_df.head()

# Preparing data for machine learning model by splitting into training and testing sets

from sklearn.model_selection import train_test_split

# Define features (X) and target variable (y) for the model

X = model_df[['budget_usd', 'sentiment_encoded', 'kpi_value']]
y = model_df['total_risk_score']

# Split the data into training (70%) and testing (30%) sets

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

print(X_train.shape, X_test.shape)

# Training a simple predictive model (Decision Tree Classifier)
# Import the DecisionTreeClassifier model from scikit-learn

from sklearn.tree import DecisionTreeClassifier

# Initialize a Decision Tree Classifier model

clf = DecisionTreeClassifier(random_state=42)

clf.fit(X_train, y_train)

# Evaluating the performance of the trained Decision Tree model

from sklearn.metrics import classification_report, confusion_matrix

# Using the trained model to make predictions on the test set features
y_pred = clf.predict(X_test)

# Evaluating the model using standard classification metrics:

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Save the processed DataFrames to CSV files

merged_df.to_csv('clean_zombie_dataset.csv', index=False)

model_df.to_csv('risk_scored_dataset.csv', index=False)